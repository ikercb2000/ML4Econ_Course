{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ML-AI4Econ Course**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download the different packages needed for this part of the course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import sklearn.datasets as data\n",
    "from sklearn.linear_model import LinearRegression, QuantileRegressor, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also set a random seed for reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create some useful functions for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coef(model):\n",
    "    return np.concatenate(([model.intercept_], model.coef_))\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    medae = median_absolute_error(y_true, y_pred)\n",
    "    return mse, rmse, mae, r2, medae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### On Prediction Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before going on, we need to talk about the different metrics that we will use to evaluate whether our model is good enough.\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "The Mean Squared Error measures the average of the squared differences between the actual values (\\(y_i\\)) and the predicted values (\\(\\hat{y}_i\\)). It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where \\(n\\) is the number of observations.\n",
    "\n",
    "**Root Mean Squared Error (RMSE)**\n",
    "\n",
    "The RMSE is the square root of the MSE, providing an error measure in the same units as the target variable. The formula is:\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "\n",
    "The Mean Absolute Error calculates the average of the absolute differences between the actual and predicted values:\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left|y_i - \\hat{y}_i\\right|\n",
    "$$\n",
    "\n",
    "This metric is less sensitive to outliers compared to the MSE.\n",
    "\n",
    "**R-squared ($R^2$)**\n",
    "\n",
    "The $R^2$ score represents the proportion of variance in the dependent variable that is explained by the model. It is computed as:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "where $\\bar{y}$ is the mean of the observed values. An $R^2$ value of 1 indicates perfect prediction, whereas a value closer to 0 indicates that the model does not explain much of the variance.\n",
    "\n",
    "**Median Absolute Error (MedAE)**\n",
    "\n",
    "The Median Absolute Error is the median of all absolute differences between the predicted and actual values:\n",
    "\n",
    "$$\n",
    "\\text{MedAE} = \\text{median} \\left( \\left|y_i - \\hat{y}_i\\right| \\right)\n",
    "$$\n",
    "\n",
    "This metric provides a robust measure of the typical error, particularly useful when the data contains outliers.\n",
    "\n",
    "\n",
    "These metrics are used for regression, but we can find different ones for classification tasks as well.\n",
    "\n",
    "**Accuracy**\n",
    "\n",
    "Accuracy measures the proportion of correct predictions out of all predictions made. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "$$\n",
    "\n",
    "**Precision**\n",
    "\n",
    "Precision measures the proportion of true positive predictions out of all positive predictions made by the model. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "$$\n",
    "\n",
    "**Recall**\n",
    "\n",
    "Recall (also known as Sensitivity or True Positive Rate) measures the proportion of actual positives that were correctly identified by the model. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "$$\n",
    "\n",
    "**F1 Score**\n",
    "\n",
    "The F1 Score is the harmonic mean of precision and recall, providing a balance between the two. It is defined as:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "This metric is particularly useful when the class distribution is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen what multivariate statistics and statistical learning theory can offer, we will go for linear models. The most important ones will be linear regression, logistic regression and LDA. We now start by first importing the data we will use throughout all the notebook, which is the \"Boston Housing Prices\" dataset, which has different variables related to housing characteristics in Boston and the price of the different houses in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = data.fetch_openml(name=\"Boston\",version=1,as_frame=True)\n",
    "\n",
    "print(\"This is our basic dataset information:\\n\\n\",boston.DESCR,\"\\n\")\n",
    "X = boston.data\n",
    "print(\"\\nThese are the original explanatory variables:\\n\\n\",X,\"\\n\")\n",
    "y = boston.target\n",
    "print(\"\\nThese are the prices for each observation (the target variable):\\n\\n\",y,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are categorical variables, so we need to do some transformation! We will do so with one-hot encoding, which is basically to include binary variables for all categories but a base one (in this case, the first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(drop=\"first\")\n",
    "cat_encoded = encoder.fit_transform(X[[\"CHAS\",\"RAD\"]])\n",
    "cat_encoded = cat_encoded.toarray()\n",
    "encoded_columns = encoder.get_feature_names_out([\"CHAS\", \"RAD\"])\n",
    "\n",
    "x_cols = list(X.columns)\n",
    "x_cols.pop(3)\n",
    "x_cols.pop(7)\n",
    "X = X[x_cols]\n",
    "\n",
    "cat_df = pd.DataFrame(cat_encoded, columns=encoded_columns, index=X.index)\n",
    "X = pd.concat([X,cat_df],axis=1)\n",
    "print(\"\\nAfter encoding, these are the variables:\\n\\n\",X,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are interested in prediction, let us divide the dataset into train and test samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we focus first on the simple multivariate regression model. We try estimating different linear regression models, first starting with the most basic one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression(n_jobs=2)  # We create first the object that will allow us to create the regression\n",
    "lin_reg.fit(X_train,y_train) # fit the model through L2\n",
    "\n",
    "coef = lin_reg.coef_ # get coefficients\n",
    "intercept = lin_reg.intercept_ # get intercept\n",
    "\n",
    "X_design = np.hstack([np.ones((X_train.shape[0], 1)), X_train.values]) # obtains the matrix with a column of ones\n",
    "p = X_design.shape[1] # obtain number of variables\n",
    "n = X_design.shape[0] # obtains number of observations\n",
    "\n",
    "y_pred = lin_reg.predict(X=X_train) # get predictions for each X in the sample\n",
    "residuals = y_train - y_pred # obtain residuals\n",
    "\n",
    "sigma2 = np.sum(residuals**2) / (n - p) # compute variance\n",
    "\n",
    "XtX_inv = np.linalg.inv(X_design.T @ X_design) # compute standard errors through formula\n",
    "std_errors = np.sqrt(sigma2 * np.diag(XtX_inv))\n",
    "\n",
    "t_stats = np.concatenate(([intercept], coef)) / std_errors\n",
    "p_values = 2 * (1 - stats.t.cdf(np.abs(t_stats), df=(n - p)))\n",
    "\n",
    "param_names = get_coef(lin_reg)\n",
    "results_ols = pd.DataFrame({\n",
    "    'Coefficient': np.concatenate(([intercept], coef)),\n",
    "    'Std_Error': std_errors,\n",
    "    't_stat': t_stats,\n",
    "    'p_value': p_values\n",
    "}, index=param_names) # create dataframe to visualize parameters\n",
    "\n",
    "print(\"L2 Loss Model (Ordinary Least Squares) results:\\n\")\n",
    "print(results_ols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what happens when we change the loss function to an L1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_reg = QuantileRegressor(quantile=0.5, alpha=0, solver='highs')\n",
    "l1_reg.fit(X_train, y_train)\n",
    "coef_l1 = l1_reg.coef_\n",
    "intercept_l1 = l1_reg.intercept_\n",
    "\n",
    "results_l1 = pd.DataFrame({\n",
    "    'Coefficient': np.concatenate(([intercept_l1], coef_l1))\n",
    "}, index=param_names)\n",
    "\n",
    "print(\"\\nL1 Model (Quantile Regression at 0.5) Coefficients results:\")\n",
    "print(results_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute the L1 loss results, there is no closed formula for the coefficients, and we cannot derive the standard deviation of our coefficients. What can we do? Are we gonna be happy with just some numbers?\n",
    "\n",
    "In this case, we can use a great statistical sampling mechanism that allows us to study variability with relatively small datasets and which is fundamental for many machine learning algorithms: we will use **bootstrap** for obtaining the variance of our coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1000 # number of boostrap iterations\n",
    "n = X_train.shape[0] # number of obs\n",
    "\n",
    "param_names = ['Intercept'] + list(X_train.columns)\n",
    "p = len(param_names) # number of variables\n",
    "coefs_bootstrap = np.zeros((B, p)) # create a matrix to save results!\n",
    "\n",
    "for b in range(B):\n",
    "    indices = np.random.choice(n, n, replace=True) # sample indices with replacement\n",
    "    X_boot = X.iloc[indices] # obtain the values of the variables for the group of indices\n",
    "    y_boot = y.iloc[indices]\n",
    "    \n",
    "    model_boot = QuantileRegressor(quantile=0.5, alpha=0, solver='highs')\n",
    "    model_boot.fit(X_boot, y_boot) # we fit the model with these values\n",
    "    \n",
    "    coef_boot = np.concatenate(([model_boot.intercept_], model_boot.coef_)) # combine coefficients\n",
    "    coefs_bootstrap[b, :] = coef_boot\n",
    "\n",
    "std_errors_boot = np.std(coefs_bootstrap, axis=0)  # compute the standard error\n",
    "\n",
    "l1_reg = QuantileRegressor(quantile=0.5, alpha=0, solver='highs')\n",
    "l1_reg.fit(X_train, y_train)\n",
    "coef_l1 = np.concatenate(([l1_reg.intercept_], l1_reg.coef_))\n",
    "\n",
    "t_stats_l1 = coef_l1 / std_errors_boot\n",
    "p_values_l1 = 2 * (1 - stats.norm.cdf(np.abs(t_stats_l1)))\n",
    "\n",
    "results_l1_boot = pd.DataFrame({\n",
    "    'Coefficient': coef_l1,\n",
    "    'Bootstrap_Std_Error': std_errors_boot,\n",
    "    't_stat': t_stats_l1,\n",
    "    'p_value': p_values_l1\n",
    "}, index=param_names)\n",
    "\n",
    "print(\"=== L1 Model (Quantile Regression) with Bootstrap Inference ===\")\n",
    "print(results_l1_boot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we cannot plot the models, let us compare the coefficients visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(param_names))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, results_ols['Coefficient'], width, label='OLS (L2)')\n",
    "ax.bar(x + width/2, results_l1['Coefficient'], width, label='L1 (Quantile)')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(param_names, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Coefficient Value\")\n",
    "ax.set_title(\"Comparison of Regression Coefficients: OLS (L2) vs L1\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to test the generalization ability of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_ols = lin_reg.predict(X_test)\n",
    "y_pred_test_l1 = l1_reg.predict(X_test)\n",
    "\n",
    "metrics_ols = evaluate_model(y_test, y_pred_test_ols)\n",
    "metrics_l1 = evaluate_model(y_test, y_pred_test_l1)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'R2', 'Median AE'],\n",
    "    'OLS': metrics_ols,\n",
    "    'L1': metrics_l1\n",
    "})\n",
    "print(\"\\nEvaluation Metrics on Test Set:\\n\")\n",
    "print(metrics_df,\"\\n\")\n",
    "\n",
    "# Plot Actual vs Predicted\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test_ols, label='L2', alpha=0.7)\n",
    "plt.scatter(y_test, y_pred_test_l1, label='L1', alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Predicted vs Actual Prices on Test Set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we able to improve these results by regularizing the models? Let us try now! In this exercise we will use different regularization techniques to compare the results with the ones obtained before. First, we estimate the new models and then we do a similar procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha=1.0, random_state=42)  # L2 regularization\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "lasso_model = Lasso(alpha=0.1, random_state=42, max_iter=10000)  # L1 regularization\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "elastic_model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=10000)\n",
    "elastic_model.fit(X_train, y_train)\n",
    "\n",
    "coef_ols = get_coef(lin_reg)\n",
    "coef_ridge = get_coef(ridge_model)\n",
    "coef_lasso = get_coef(lasso_model)\n",
    "coef_elastic = get_coef(elastic_model)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'OLS': coef_ols,\n",
    "    'Ridge': coef_ridge,\n",
    "    'Lasso': coef_lasso,\n",
    "    'ElasticNet': coef_elastic\n",
    "}, index=param_names)\n",
    "\n",
    "print(\"\\nCoefficients from Different Models:\\n\")\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the coefficients through a visual comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(param_names))  # one position for each parameter\n",
    "width = 0.2  # width for each bar\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.bar(x - 1.5*width, coef_df['OLS'], width, label='OLS')\n",
    "ax.bar(x - 0.5*width, coef_df['Ridge'], width, label='Ridge')\n",
    "ax.bar(x + 0.5*width, coef_df['Lasso'], width, label='Lasso')\n",
    "ax.bar(x + 1.5*width, coef_df['ElasticNet'], width, label='ElasticNet')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(param_names, rotation=45, ha='right')\n",
    "ax.set_ylabel(\"Coefficient Value\")\n",
    "ax.set_title(\"Comparison of Regression Coefficients\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, how did our regularized models do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ols = lin_reg.predict(X_test)\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "y_pred_elastic = elastic_model.predict(X_test)\n",
    "\n",
    "metrics_ols = evaluate_model(y_test, y_pred_ols)\n",
    "metrics_ridge = evaluate_model(y_test, y_pred_ridge)\n",
    "metrics_lasso = evaluate_model(y_test, y_pred_lasso)\n",
    "metrics_elastic = evaluate_model(y_test, y_pred_elastic)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'R2', 'Median AE'],\n",
    "    'OLS': metrics_ols,\n",
    "    'Ridge': metrics_ridge,\n",
    "    'Lasso': metrics_lasso,\n",
    "    'ElasticNet': metrics_elastic\n",
    "})\n",
    "print(\"\\n=== Evaluation Metrics on Test Set ===\")\n",
    "print(metrics_df,\"\\n\")\n",
    "\n",
    "# Plot of Actual vs Predicted\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_ols, label='OLS', alpha=0.7)\n",
    "plt.scatter(y_test, y_pred_ridge, label='Ridge', alpha=0.7)\n",
    "plt.scatter(y_test, y_pred_lasso, label='Lasso', alpha=0.7)\n",
    "plt.scatter(y_test, y_pred_elastic, label='ElasticNet', alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Predicted vs Actual Prices on Test Set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there anything we can still do to get a good generalization, apart from changing the loss function we estimate the model with? Yes. We can still try to find a better model by using a technique called **cross-validation** for **hyperparameter tuning**. This will allow us to obtain a \"best\" model for our regularized models and assess whether our generalization ability was good enouigh even before testing the data.\n",
    "\n",
    "---\n",
    "\n",
    "### On Cross-validation\n",
    "\n",
    "Cross validation involves partitioning the dataset into several folds. In each iteration, one fold is used as the test set while the remaining folds are used for training. This process is repeated until each fold has served as the test set exactly once. Some common types of cross-validation are the following:\n",
    "\n",
    "- **K-Fold Cross Validation:**  \n",
    "  The data is divided into K equal parts (folds). The model is trained K times, each time leaving out one of the folds as the test set. The performance is then averaged over the K iterations.\n",
    "\n",
    "- **Leave-One-Out Cross Validation (LOOCV):**  \n",
    "  A special case of K-Fold where K is equal to the number of observations. Each observation is used once as the test set, while the remaining observations form the training set.\n",
    "\n",
    "- **Stratified K-Fold Cross Validation:**  \n",
    "  Used mainly for classification, this method ensures that each fold has approximately the same percentage of samples of each target class as the complete dataset.\n",
    "\n",
    "Cross validation provides a more reliable estimate of model performance compared to a single train/test split, and also allows choosing between hyperparameters. It reduces the risk of overfitting and ensures that the model's evaluation is not dependent on one particular split of the data. The practical process would be the following:\n",
    "\n",
    "1. Divide the dataset into K folds.\n",
    "2. For each fold:\n",
    "   - Train the model on the remaining K-1 folds.\n",
    "   - Test the model on the current fold.\n",
    "3. Calculate the performance metric for each fold.\n",
    "4. Average the metrics across all folds to obtain the final performance measure.\n",
    "5. If the model has hyperparameters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_metrics = evaluate_model(y_test, y_pred_ridge)\n",
    "lasso_metrics = evaluate_model(y_test, y_pred_lasso)\n",
    "\n",
    "ridge_params = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "lasso_params = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]}\n",
    "\n",
    "ridge_cv = GridSearchCV(Ridge(random_state=42), ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "best_ridge = ridge_cv.best_estimator_\n",
    "\n",
    "mean_train_score = -ridge_cv.cv_results_[\"mean_test_score\"][ridge_cv.best_index_]\n",
    "print(\"Mean Training MSE from CV (Ridge): {:.4f}\".format(mean_train_score),\"\\n\\n\")\n",
    "\n",
    "lasso_cv = GridSearchCV(Lasso(random_state=42), lasso_params, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "best_lasso = lasso_cv.best_estimator_\n",
    "\n",
    "mean_train_score = -lasso_cv.cv_results_[\"mean_test_score\"][lasso_cv.best_index_]\n",
    "print(\"Mean Training MSE from CV (Lasso): {:.4f}\".format(mean_train_score),\"\\n\\n\")\n",
    "\n",
    "best_ridge_pred = best_ridge.predict(X_test)\n",
    "best_lasso_pred = best_lasso.predict(X_test)\n",
    "\n",
    "best_ridge_metrics = evaluate_model(y_test, best_ridge_pred)\n",
    "best_lasso_metrics = evaluate_model(y_test, best_lasso_pred)\n",
    "\n",
    "models = [\n",
    "    'Ridge (default)',\n",
    "    'Lasso (default)',\n",
    "    'Ridge (CV tuned)',\n",
    "    'Lasso (CV tuned)'\n",
    "]\n",
    "\n",
    "mse_values = [\n",
    "    ridge_metrics[0],\n",
    "    lasso_metrics[0],\n",
    "    best_ridge_metrics[0],\n",
    "    best_lasso_metrics[0],\n",
    "]\n",
    "\n",
    "mae_values = [\n",
    "    ridge_metrics[2],\n",
    "    lasso_metrics[2],\n",
    "    best_ridge_metrics[2],\n",
    "    best_lasso_metrics[2]\n",
    "]\n",
    "\n",
    "r2_values = [\n",
    "    ridge_metrics[3],\n",
    "    lasso_metrics[3],\n",
    "    best_ridge_metrics[3],\n",
    "    best_lasso_metrics[3]\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'MSE': mse_values,\n",
    "    'MAE': mae_values,\n",
    "    'R2': r2_values\n",
    "})\n",
    "\n",
    "print(\"Test Set Performance Comparison:\\n\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphically, we can spot the changes that happened here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_ridge_cv = get_coef(best_ridge)\n",
    "coef_lasso_cv = get_coef(best_lasso)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Ridge (default)': coef_ridge,\n",
    "    'Lasso (default)': coef_lasso,\n",
    "    'Ridge (CV tuned)': coef_ridge_cv,\n",
    "    'Lasso (CV tuned)': coef_lasso_cv\n",
    "}, index=['Intercept'] + list(X_train.columns))\n",
    "\n",
    "coef_df.plot(kind='bar', figsize=(14,7))\n",
    "plt.ylabel(\"Coefficient Value\")\n",
    "plt.title(\"Comparison of Coefficients for Different Models\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also take a look at the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_ridge, label='Ridge', alpha=0.7)\n",
    "plt.scatter(y_test, y_pred_lasso, label='Lasso', alpha=0.7)\n",
    "plt.scatter(y_test, best_ridge_pred, label='Ridge CV', alpha=0.7)\n",
    "plt.scatter(y_test, best_lasso_pred, label='Lasso CV', alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Predicted vs Actual Prices on Test Set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen a lot of stuff! In fact, we have seen techniques for **evaluation**, **model selection**, **data transformation** and **regularization**. Therefore, these are now assumed to be known, and we will apply them regularly without the need of comparing different possibilities from now on. We can now continue to linear models for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want a linear model that can work for classification, given that the linear regression model is not suited for this purpose. One of the best optios is the logistic regression, which basically is a generalized linear model that allows to classify different observations based on the probability given by the model to each of the possible classes or categories. In this case, we will not use the \"Boston Housing Prices\" dataset, but the \"Iris\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = data.fetch_openml(name='iris', version=1, as_frame=True)\n",
    "iris_desc = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"This is our basic dataset information:\\n\\n\",iris_desc.DESCR,\"\\n\")\n",
    "print(\"Features:\\n\\n\")\n",
    "print(X_iris)\n",
    "print(\"\\n\\nTarget:\\n\\n\")\n",
    "print(y_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now divide train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we estimate our logistic regression model with our train data and evaluate the performance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model using classification report and confusion matrix\n",
    "\n",
    "print(\"\\nClassification Report:\\n\\n\")\n",
    "print(classification_report(y_test, y_pred),\"\\n\")\n",
    "print(\"\\nConfusion Matrix:\\n\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at how we are assigning each observation to a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = logreg.predict_proba(X_test)\n",
    "prob_df = pd.DataFrame(probabilities, columns=[f\"Probability(Class {i})\" for i in range(probabilities.shape[1])])\n",
    "print(\"Predicted class probabilities for some test observations:\\n\")\n",
    "print(prob_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "\n",
    "A different approach would be to use linear discriminant analysis. In this case, we try to infer some Bayesian decision boundaries from the data among categories to classify different observations, assuming normality, and we play with Bayes Theorem in order to provide a posterior probability of pertaining to one class for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = X_iris.iloc[:, 2:4]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_plot, y_iris, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "lda.fit(X_train, y_train)\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lda = lda.predict(X_test)\n",
    "y_pred_qda = qda.predict(X_test)\n",
    "\n",
    "# Evaluate the model using classification report and confusion matrix\n",
    "\n",
    "print(\"\\nClassification Report for LDA:\\n\\n\")\n",
    "print(classification_report(y_test, y_pred_lda),\"\\n\")\n",
    "print(\"\\nConfusion Matrix for LDA:\\n\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_lda))\n",
    "\n",
    "print(\"\\nClassification Report for QDA:\\n\\n\")\n",
    "print(classification_report(y_test, y_pred_qda),\"\\n\")\n",
    "print(\"\\nConfusion Matrix for QDA:\\n\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_qda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can observe some observations' probabilities of pertaining to one group or another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_lda = lda.predict_proba(X_test)\n",
    "proba_qda = qda.predict_proba(X_test)\n",
    "\n",
    "\n",
    "print(\"Predicted class probabilities (LDA) for first 5 test samples:\")\n",
    "print(proba_lda[:5])\n",
    "print(\"\\nPredicted class probabilities (QDA) for first 5 test samples:\")\n",
    "print(proba_qda[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LDA and QDA, the best idea is to visualize what the decision boundary looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = data.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "X_plot = X_iris.iloc[:, 2:4]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_plot, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "lda.fit(X_train, y_train)\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "x_min, x_max = X_train.iloc[:, 0].min() - 1, X_train.iloc[:, 0].max() + 1\n",
    "y_min, y_max = X_train.iloc[:, 1].min() - 1, X_train.iloc[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "Z_lda = lda.predict(grid_points).reshape(xx.shape)\n",
    "Z_qda = qda.predict(grid_points).reshape(xx.shape)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6), constrained_layout=True)\n",
    "cmap = plt.cm.Paired\n",
    "\n",
    "axs[0].contourf(xx, yy, Z_lda, alpha=0.3, cmap=cmap)\n",
    "sc0 = axs[0].scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_test, edgecolor='k', cmap=cmap, marker='o', s=50)\n",
    "axs[0].set_xlabel(\"Petal length (cm)\")\n",
    "axs[0].set_ylabel(\"Petal width (cm)\")\n",
    "axs[0].set_title(\"LDA Decision Boundaries (Test Data)\")\n",
    "handles0, _ = sc0.legend_elements()\n",
    "axs[0].legend(handles=handles0, labels=list(target_names), title=\"Classes\")\n",
    "\n",
    "axs[1].contourf(xx, yy, Z_qda, alpha=0.3, cmap=cmap)\n",
    "sc1 = axs[1].scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_test, edgecolor='k', cmap=cmap, marker='o', s=50)\n",
    "axs[1].set_xlabel(\"Petal length (cm)\")\n",
    "axs[1].set_ylabel(\"Petal width (cm)\")\n",
    "axs[1].set_title(\"QDA Decision Boundaries (Test Data)\")\n",
    "handles1, _ = sc1.legend_elements()\n",
    "axs[1].legend(handles=handles1, labels=list(target_names), title=\"Classes\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
