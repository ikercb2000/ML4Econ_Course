{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ML-AI4Econ Course**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download the different packages needed for this part of the course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also set a random seed for reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Learning Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing a little statistics refresher, let us just practically understand the implications of statistical learninng theory for ML\\&AI. First of all, we will simulate a sinusoidal function, and we will add some noise to the observations, so that we have a kind of noisy dataset that has an underlying sinusoidal function in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "X = np.linspace(-3, 3, n_samples).reshape(-1, 1)\n",
    "y_true = np.sin(X).ravel()\n",
    "noise = np.random.normal(0, 0.15, size=y_true.shape)\n",
    "y_observed = y_true + noise\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_observed, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider to fit them by through a polinomial function with different degrees. As you know, a higher degree makes our function more complex because it allows for more \"twists\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 3, 10]\n",
    "models = [make_pipeline(PolynomialFeatures(degree), LinearRegression()) for degree in degrees]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit and visualize the different models for different complexity levels. We compare the training and the test errors for each fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "for i, model in enumerate(models):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_error = mean_squared_error(y_train, y_train_pred)\n",
    "    test_error = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Model with degree {degrees[i]}:\")\n",
    "    print(f\"Empirical Risk (Training MSE): {train_error:.4f}\")\n",
    "    print(f\"True Risk (Test MSE): {test_error:.4f}\\n\")\n",
    "    \n",
    "    plt.subplot(1, len(degrees), i+1)\n",
    "    plt.scatter(X_train, y_train, label=\"Training Data\", color='blue', alpha=0.5)\n",
    "    plt.scatter(X_test, y_test, label=\"Test Data\", color='red', alpha=0.5)\n",
    "    \n",
    "    X_range = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "    y_range_pred = model.predict(X_range)\n",
    "    plt.plot(X_range, y_range_pred, label=f\"Degree {degrees[i]}\", color='black')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Polynomial Degree {degrees[i]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could we observe?\n",
    "\n",
    "- Lower-degree models (e.g., linear) have high bias but lower variance, leading to underfitting.\n",
    "- Higher-degree models (e.g., degree 10) fit training data well but generalize poorly due to overfitting.\n",
    "- The best model balances empirical risk and true risk, minimizing generalization error.\n",
    "\n",
    "We will talk more about this \"bias-variance trade-off\" in the future, as it is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not just constrained to regression, but to classification as well. We first use a dataset from Scikit-learn package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we consider different complexity levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 3, 6]\n",
    "models = [make_pipeline(PolynomialFeatures(degree), StandardScaler(), LogisticRegression()) for degree in degrees]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we do the same exercise as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i, model in enumerate(models):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Model with polynomial degree {degrees[i]}:\")\n",
    "    print(f\"Training Accuracy (Empirical Risk): {train_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy (True Risk): {test_accuracy:.4f}\\n\")\n",
    "\n",
    "    plt.subplot(1, len(degrees), i+1)\n",
    "    xx, yy = np.meshgrid(np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 100),\n",
    "                         np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 100))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, marker='o', label='Training Data')\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='x', label='Test Data')\n",
    "    plt.title(f\"Degree {degrees[i]}\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we observe that:\n",
    "\n",
    "- Lower-degree models (e.g., linear) may underfit the data.\n",
    "- Higher-degree models (e.g., degree 6) fit training data well and, in this case, also generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take into account these are just examples, and real-life applications are much more complex and hence effects of the complexity of the model can be more or less obvious depending on the situation and the modeller choices, and the can also have from insignificant to history-changing consequences for our applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
