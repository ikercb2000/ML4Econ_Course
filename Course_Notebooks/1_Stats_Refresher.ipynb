{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ML-AI4Econ Course**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download the different packages needed for this part of the course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_distances\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also set a random seed for reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first practice, we will generate a synthetic dataset. This dataset is being created by drawing observations from three independent standard normal distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 200\n",
    "dimensions = 3\n",
    "\n",
    "data = np.random.randn(num_samples, dimensions)  # Normal distribution\n",
    "columns = ['Feature 1', 'Feature 2', 'Feature 3']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "print(\"Example of Multivariate Data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each column has been generated by a standard normal distribution, it is like if we had a small set of random variables which for which we have been drawing as many times as the defined number of samples. Now, we can take a look at the mean vector, the covariance and the correlation matrix for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vector = df.mean()\n",
    "cov_matrix = df.cov()\n",
    "corr_matrix = df.corr()\n",
    "print(\"\\nMean Vector:\")\n",
    "print(mean_vector)\n",
    "print(\"\\nCovariance Matrix:\")\n",
    "print(cov_matrix)\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, when we deal with a multivariate setting (when we have a dataset with multiple variables), we can treat this dataset as draws from a random vector, and hence we can compute the different statistics that have been shown in the slides. We do not obtain a single measure, but a vector or matrix version of each statistic. We can now better visualize the relationships between each pair of variables and the distribution of each one of them separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, diag_kind='kde')\n",
    "plt.suptitle(\"Pairplot of Multivariate Data\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, because we can consider a dataset as a draw from a random vector, this vector has an inherent multivariate distribution, even if the marginnal distributions of each variable are different or the different variables are not related among each other. This multivariate distribution depends on more than one range of values (more than one sample space), and we can exemplify this by creating a bivariate normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = [0, 0]  # Mean\n",
    "sigma = [[1, 0.8], [0.8, 1]]  # Covariance matrix\n",
    "rv = multivariate_normal(mu, sigma)\n",
    "\n",
    "x, y = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "pos = np.dstack((x, y))\n",
    "z = rv.pdf(pos)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x, y, z, cmap='viridis')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Density')\n",
    "plt.title(\"Bivariate Normal Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe now it is a 3D shape, as we have the density in the $z$ or vertical axis, and the sample space for the random variables in the horizontal axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **distance function**, or **metric**, is a function that quantifies how far apart two points are in a given space. Formally, a function $d: X \\times X \\to \\mathbb{R}$ is called a **metric** on a set $X$ if it satisfies the following properties for all points $( x, y, z) \\in X$:\n",
    "\n",
    "- Non-negativity: $d(x, y) \\geq 0$\n",
    "\n",
    "- Identity of Indiscernibles: $d(x, y) = 0 \\quad \\text{if and only if} \\quad x = y$\n",
    "\n",
    "- Symmetry: $d(x, y) = d(y, x)$\n",
    "\n",
    "- Triangle Inequality: $d(x, z) \\leq d(x, y) + d(y, z)$\n",
    "\n",
    "These properties ensure that a distance function behaves as expected in both mathematical theory and practical applications. To get a good understanding of how we measure distance is relevant, let's check the following example: we are now sampling some random data points, and we will compute the distances through different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_points = df.iloc[:5]\n",
    "sample_df = pd.DataFrame(sample_points, columns=['Feature 1', 'Feature 2', 'Feature 3'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(sample_points.iloc[:, 0], sample_points.iloc[:, 1], c='red', label='Sample Points')\n",
    "for i, txt in enumerate(sample_points.index):\n",
    "    plt.annotate(txt, (sample_points.iloc[i, 0], sample_points.iloc[i, 1]))\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Visualization of Sample Points\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One of the most common distance metrics is the **Euclidean distance**, which is defined as:\n",
    "\n",
    "$$\n",
    "d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "where $( x, y )$ are two points in an $n$-dimensional space.\n",
    "\n",
    "This metric satisfies all the required properties and is widely used in machine learning, clustering, and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_matrix = euclidean_distances(sample_points, sample_points)\n",
    "\n",
    "print(\"\\nEuclidean Distance Matrix:\")\n",
    "print(pd.DataFrame(euclidean_matrix, index=sample_points.index, columns=sample_points.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Mahalanobis distance** is a measure of distance between a point and a distribution. Given a point $\\mathbf{x}$ and a mean vector $\\mu$, the Mahalanobis distance is defined as:\n",
    "\n",
    "$$\n",
    "d_M(\\mathbf{x}, \\mu) = \\sqrt{(\\mathbf{x} - \\mu)^T S^{-1} (\\mathbf{x} - \\mu)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x}$ is the data point (vector),\n",
    "- $\\mu$ is the mean vector of the distribution,\n",
    "- $S$ is the covariance matrix of the dataset,\n",
    "- $S^{-1}$ is the inverse of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = np.cov(sample_points, rowvar=False)\n",
    "cov_inv = np.linalg.inv(cov_matrix)\n",
    "mahalanobis_matrix = cdist(sample_points, sample_points, metric='mahalanobis', VI=cov_inv)\n",
    "print(\"\\nMahalanobis Distance Matrix:\")\n",
    "print(pd.DataFrame(mahalanobis_matrix, index=sample_df.index, columns=sample_df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Manhattan distance**, also known as the **Taxicab distance** or **L1 norm**, measures the distance between two points by summing the absolute differences of their coordinates. For two points $\\mathbf{x}$ and $\\mathbf{y}$, the **Manhattan distance** is given by:\n",
    "\n",
    "$$\n",
    "d_M(x, y) = \\sum_{i=1}^{n} |\\mathbf{x}_i - \\mathbf{y}_i|\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x}_i$ and $\\mathbf{y}_i$ are the coordinates of the points in an $n$-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_matrix = manhattan_distances(sample_points, sample_points)\n",
    "print(\"\\nManhattan Distance Matrix:\")\n",
    "print(pd.DataFrame(manhattan_matrix, index=sample_points.index, columns=sample_points.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Cosine distance** measures the **angular difference** between two vectors. It is widely used in **text similarity**, **recommendation systems**, and **high-dimensional spaces**. The **Cosine similarity** between two vectors $\\mathbf{x}$ and $\\mathbf{y}$ is given by:\n",
    "\n",
    "$$\n",
    "\\cos(\\theta) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x} \\cdot \\mathbf{y}$ is the dot product of the vectors,\n",
    "- $\\|\\mathbf{x}\\|$ and $\\|\\mathbf{y}\\|$ are the Euclidean norms (magnitudes) of the vectors.\n",
    "\n",
    "The **Cosine distance** is then defined as:\n",
    "\n",
    "$$\n",
    "d_C(x, y) = 1 - \\cos(\\theta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_matrix = cosine_distances(sample_points, sample_points)\n",
    "print(\"\\nCosine Distance Matrix:\")\n",
    "print(pd.DataFrame(cosine_matrix, index=sample_points.index, columns=sample_points.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "distance_matrices = [(euclidean_matrix, \"Euclidean Distance\"),\n",
    "                     (mahalanobis_matrix, \"Mahalanobis Distance\"), \n",
    "                     (manhattan_matrix, \"Manhattan Distance\"), \n",
    "                     (cosine_matrix, \"Cosine Distance\")]\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (matrix, title) in zip(axes, distance_matrices):\n",
    "    sns.heatmap(matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curse of dimensionality is easily one of the biggest problems in ML \\& AI. To visualize the issue, we will use the notion of distances between points and dimensions (the number of variables).Let's consider different number of dimensions and the distribution of the distances for the various sampled points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 1D Case\n",
    "high_dim_data_1D = np.random.randn(100, 1)\n",
    "distances_1D = euclidean_distances(high_dim_data_1D, high_dim_data_1D)\n",
    "mean_distance = np.mean(distances_1D)\n",
    "axes[0].scatter(high_dim_data_1D, np.zeros_like(high_dim_data_1D), c='blue')\n",
    "axes[0].set_title(\"Sample Points in 1D\")\n",
    "axes[0].set_xlabel(\"Feature 1\")\n",
    "\n",
    "axes[1].hist(distances_1D.flatten(), bins=30, alpha=0.7, color='b')\n",
    "axes[1].set_title(f\"Distance Distribution in 1D (Mean={mean_distance:.2f})\")\n",
    "axes[1].set_xlabel(\"Distance\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# 2D Case\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "high_dim_data_2D = np.random.randn(100, 2)\n",
    "distances_2D = euclidean_distances(high_dim_data_2D, high_dim_data_2D)\n",
    "mean_distance = np.mean(distances_2D)\n",
    "axes[0].scatter(high_dim_data_2D[:, 0], high_dim_data_2D[:, 1], c='blue')\n",
    "axes[0].set_title(\"Sample Points in 2D\")\n",
    "axes[0].set_xlabel(\"Feature 1\")\n",
    "axes[0].set_ylabel(\"Feature 2\")\n",
    "\n",
    "axes[1].hist(distances_2D.flatten(), bins=30, alpha=0.7, color='b')\n",
    "axes[1].set_title(f\"Distance Distribution in 2D (Mean={mean_distance:.2f})\")\n",
    "axes[1].set_xlabel(\"Distance\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, for low dimensions, points seem to cover most of the sample space. However, let's increase it to three dimensions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "high_dim_data_3D = np.random.randn(100, 3)\n",
    "distances_3D = euclidean_distances(high_dim_data_3D, high_dim_data_3D)\n",
    "mean_distance = np.mean(distances_3D)\n",
    "ax.scatter(high_dim_data_3D[:, 0], high_dim_data_3D[:, 1], high_dim_data_3D[:, 2], c='blue')\n",
    "ax.set_title(\"Sample Points in 3D\")\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")\n",
    "ax.set_zlabel(\"Feature 3\")\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.hist(distances_3D.flatten(), bins=30, alpha=0.7, color='b')\n",
    "ax2.set_title(f\"Distance Distribution in 3D (Mean={mean_distance:.2f})\")\n",
    "ax2.set_xlabel(\"Distance\")\n",
    "ax2.set_ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that the points sampled cover less three-dimensional space, even if we cannot notice a big shift in the distribution. Bigger shifts, however, are noticed when dealing with higher dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "for idx, dim in enumerate([5, 10]):\n",
    "    high_dim_data = np.random.randn(100, dim)\n",
    "    distances = euclidean_distances(high_dim_data, high_dim_data)\n",
    "    mean_distance = np.mean(distances)\n",
    "    \n",
    "    # Histogram of distances\n",
    "    axes[idx].hist(distances.flatten(), bins=30, alpha=0.7, color='b')\n",
    "    axes[idx].set_title(f\"Distance Distribution in {dim}D (Mean={mean_distance:.2f})\")\n",
    "    axes[idx].set_xlabel(\"Distance\")\n",
    "    axes[idx].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are a bunch of exercises for testing your understanding. Please do not use LLMs or ChatGPT to answer them (unless you are explicitly ordered to do so), but look at the documentation of different packages and websites for a better control of Python packages for ML \\& AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1: Understanding Mean and Covariance**\n",
    "**Objective:** Compute and interpret the mean vector and covariance matrix.\n",
    "\n",
    "- Generate a **3-dimensional multivariate dataset** with **200 samples**.\n",
    "- Compute the **mean vector** and **covariance matrix**.\n",
    "- Explain in **your own words** what the covariance matrix represents. Why are there repeated elements?\n",
    "\n",
    "### **Exercise 2: Multivariate Normal Distribution**\n",
    "**Objective:** Visualizing and understanding multivariate distributions.\n",
    "\n",
    "- Generate a **bivariate normal distribution** with a specified mean vector and covariance matrix.\n",
    "- Plot its **contour** and **3D surface plot**.\n",
    "- Modify the **covariance structure** and observe how the shape of the distribution changes.\n",
    "- What happens if the covariance is **0**? What if it is **negative**?\n",
    "\n",
    "### **Exercise 3: Random Vectors and Transformations**\n",
    "**Objective:** Understanding transformations of multivariate data.\n",
    "\n",
    "- Generate a **5-sample** dataset from a 3D normal distribution.\n",
    "- Compute the **Mahalanobis distance** of each sample from the mean.\n",
    "- Apply a **linear transformation** (e.g., a matrix multiplication) to the dataset.\n",
    "- Compute the **new covariance matrix**. What changed? Why?\n",
    "\n",
    "### **Exercise 4: Compute and Compare Distance Metrics**\n",
    "**Objective:** Understanding how different distance metrics behave.\n",
    "\n",
    "- What key differences do you notice? Which metrics give the largest/smallest values? Why?\n",
    "- Could you explain the intuition on why the metric is useful? In what aspects do the different metrics proposed differ? When are they useful?\n",
    "\n",
    "### **Exercise 5: Visualizing Distance Matrices**\n",
    "**Objective:** Learn how to interpret and visualize distances.\n",
    "\n",
    "- Using the distance matrices from **Exercise 4**, create **heatmaps**.\n",
    "- Compare the **patterns** in different metrics.\n",
    "- If a dataset is **centered** (mean = 0) but has **highly correlated features**, which distance metric would be most affected?\n",
    "\n",
    "### **Exercise 6: Detecting Outliers with Mahalanobis Distance**\n",
    "**Objective:** Apply Mahalanobis distance in anomaly detection.\n",
    "\n",
    "- Generate a **3D dataset with 100 samples** from a normal distribution.\n",
    "- Add **three outliers** (points that are far from the mean).\n",
    "- Compute **Mahalanobis distances** for all points.\n",
    "- Identify **outliers** based on a threshold.\n",
    "- Compare with **Euclidean distance**: Which method better detects outliers? Why? What is the intuition behind the Mahalanobis distance?\n",
    "\n",
    "### **Exercise 7: Exploring High-Dimensional Space**\n",
    "**Objective:** Observe how distances behave as dimensions increase.\n",
    "\n",
    "- Generate **datasets** with 100 points for:\n",
    "  - **1D, 2D, 3D, 5D, and 10D**.\n",
    "- Compute the **mean pairwise distance** using different distances for each dataset.\n",
    "- Plot **dimension vs. mean distance**.\n",
    "- What do you notice? What does this tell you about high-dimensional data?\n",
    "\n",
    "### **Exercise 8: Distance Concentration in High Dimensions**\n",
    "**Objective:** Understanding why high-dimensional distances become uniform.\n",
    "\n",
    "- Generate **1000 points in 100 dimensions**.\n",
    "- Compute **all pairwise Euclidean distances**.\n",
    "- Plot a histogram of the distances.\n",
    "- What shape does the histogram take? How does this relate to the **curse of dimensionality**?\n",
    "\n",
    "### **Exercise 9: Cosine Similarity vs. Euclidean Distance in High Dimensions**\n",
    "**Objective:** Compare the behavior of different metrics in high dimensions.\n",
    "\n",
    "- Generate **1000 points in 50D space**.\n",
    "- Compute both **Euclidean distances** and **Cosine distances**.\n",
    "- Compare their **distributions**.\n",
    "- Which metric becomes **less meaningful** in high dimensions? Why?\n",
    "\n",
    "### **Exercise 10: High-Dimensional Data Visualization**\n",
    "**Objective:** Learn how to visualize high-dimensional data.\n",
    "\n",
    "- Generate **a 10-dimensional dataset** with 1000 samples.\n",
    "- Reduce its dimensions using **Principal Component Analysis (PCA)** to **2D and 3D**.\n",
    "- Plot the **2D and 3D projections**.\n",
    "- Discuss: How much information do we lose in these projections?\n",
    "\n",
    "### **Extra Exercise: Real Challenges**\n",
    "**Objective:** Learn how to do all this stuff with real data.\n",
    "\n",
    "- Get a real-world dataset.\n",
    "- Explore different visualization and descriptive statistics techniques.\n",
    "- Interpret the results you obtain to explain useful insights of your dataset.\n",
    "- Try to implement some ML methods for your own (help yourself with ChatGPT if needed) and obtain test metrics.\n",
    "- Which ones work best in your setting? Is this what you expected? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
